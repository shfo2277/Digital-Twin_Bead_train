#/home/ho/BEADtrain/Calibration/handeye_calib_node.py
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node

from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge

import tf2_ros
from tf2_ros.transform_listener import TransformListener
from geometry_msgs.msg import TransformStamped
from rclpy.time import Time

import numpy as np
import cv2
from cv2 import aruco


class HandEyeCalibNode(Node):
    def __init__(self):
        super().__init__('handeye_calib_node')

        # -----------------------------
        # ğŸ”§ ê¸°ë³¸ í”„ë ˆì„ / íŒŒë¼ë¯¸í„° ì„¤ì •
        # -----------------------------
        self.base_frame = 'base_link'
        self.tcp_frame  = 'tool0'
        self.camera_frame = 'camera_color_optical_frame'


        # ğŸ”§ ë„¤ê°€ ë§Œë“  3Ã—3 ì•„ë£¨ì½” ë³´ë“œ ì„¤ì • (4cm, gap 1cm)
        self.marker_length = 0.04              # [m] ë§ˆì»¤ í•œ ë³€ = 4cm
        self.marker_separation = 0.01          # [m] ë§ˆì»¤ ì‚¬ì´ ê°„ê²© = 1cm
        self.rows = 3
        self.cols = 3

        self.num_samples_target = 20
        self.sample_interval_sec = 0.7

        # -----------------------------
        # ğŸ”§ TF ë²„í¼ & ë¦¬ìŠ¤ë„ˆ
        # -----------------------------
        self.tf_buffer = tf2_ros.Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)

        # -----------------------------
        # ğŸ”§ ì¹´ë©”ë¼ êµ¬ë…
        # -----------------------------
        self.bridge = CvBridge()
        self.image_sub = self.create_subscription(
            Image,
            '/camera/camera/color/image_raw',
            self.image_callback,
            10
        )

        self.cinfo_sub = self.create_subscription(
            CameraInfo,
            '/camera/camera/color/camera_info',
            self.cinfo_callback,
            10
        )

        # ì¹´ë©”ë¼ ë‚´ì°¸/ì™œê³¡
        self.camera_matrix = None
        self.dist_coeffs = None

        # -----------------------------
        # ğŸ”§ ArUco ë”•ì…”ë„ˆë¦¬ & GridBoard
        # -----------------------------
        self.aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_5X5_100)
        if hasattr(aruco, "DetectorParameters_create"):
            self.aruco_params = aruco.DetectorParameters_create()
        else:
            self.aruco_params = aruco.DetectorParameters()
        self.board = aruco.GridBoard(
            (self.cols, self.rows),           # (markersX, markersY)
            self.marker_length,               # markerLength
            self.marker_separation,           # markerSeparation
            self.aruco_dict                   # dictionary
        )

        # ìƒ˜í”Œ ì €ì¥ ë¦¬ìŠ¤íŠ¸
        self.R_gripper2base_list = []
        self.t_gripper2base_list = []
        self.R_target2cam_list = []
        self.t_target2cam_list = []

        self.last_sample_time = self.get_clock().now()

        self.get_logger().info('âœ… Hand-Eye GridBoard calibration node started.')
        self.get_logger().info(f'   base_frame: {self.base_frame}, tcp_frame: {self.tcp_frame}')
        self.get_logger().info('   3x3 GridBoard, marker=0.04m, gap=0.01m, DICT_5X5_100 ì‚¬ìš© ì¤‘')


    # -----------------------------
    # ğŸ“· CameraInfo ì½œë°±
    # -----------------------------
    def cinfo_callback(self, msg: CameraInfo):
        if self.camera_matrix is None:
            K = np.array(msg.k, dtype=np.float64).reshape(3, 3)
            D = np.array(msg.d, dtype=np.float64)
            self.camera_matrix = K
            self.dist_coeffs = D
            self.get_logger().info('ğŸ“· Camera info received & stored.')

    # -----------------------------
    # ğŸ“· ì´ë¯¸ì§€ ì½œë°±
    # -----------------------------
    def image_callback(self, msg: Image):
        # CameraInfo ì¤€ë¹„ ì•ˆëìœ¼ë©´ skip
        if self.camera_matrix is None or self.dist_coeffs is None:
            return
    
        # ROS Image â†’ OpenCV
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
    
        # 1) detect markers
        corners, ids, _ = aruco.detectMarkers(
            gray, self.aruco_dict, parameters=self.aruco_params
        )
        if ids is None or len(ids) == 0:
            cv2.imshow("Calibration", cv_image)
            cv2.waitKey(1)
            return
    
        # 2) estimate pose
        rvec_init = np.zeros((3, 1), dtype=np.float32)
        tvec_init = np.zeros((3, 1), dtype=np.float32)
    
        retval, rvec, tvec = aruco.estimatePoseBoard(
            corners, ids, self.board,
            self.camera_matrix, self.dist_coeffs,
            rvec_init, tvec_init
        )
        if retval <= 0:
            cv2.imshow("Calibration", cv_image)
            cv2.waitKey(1)
            return
    
        # pose OK
        R_target2cam, _ = cv2.Rodrigues(rvec)
        t_target2cam = tvec.reshape(3, 1)
    
        # 3) TF lookup (base -> tcp)
        try:
            now = Time()
            trans = self.tf_buffer.lookup_transform(
                self.base_frame, self.tcp_frame, now
            )
        except Exception as e:
            self.get_logger().warn(f"TF lookup failed: {e}")
            cv2.imshow("Calibration", cv_image)
            cv2.waitKey(1)
            return
    
        q = trans.transform.rotation
        t = trans.transform.translation
    
        T_base_tcp = self.quaternion_to_matrix([q.x, q.y, q.z, q.w])
        T_base_tcp[0, 3] = t.x
        T_base_tcp[1, 3] = t.y
        T_base_tcp[2, 3] = t.z
    
        R_gripper2base = T_base_tcp[0:3, 0:3]
        t_gripper2base = T_base_tcp[0:3, 3].reshape(3, 1)
    
        # -----------------------------
        # â–¶â–¶ HERE: S í‚¤ ëˆ„ë¥´ë©´ ìƒ˜í”Œ ì €ì¥
        # -----------------------------
        cv2.putText(cv_image, "Press 's' to save sample", (20, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
    
        cv2.imshow("Calibration", cv_image)
        key = cv2.waitKey(1) & 0xFF
    
        if key == ord('s'):  # â† S í‚¤ë¡œ ì €ì¥
            if len(self.R_gripper2base_list) < self.num_samples_target:
                self.R_gripper2base_list.append(R_gripper2base)
                self.t_gripper2base_list.append(t_gripper2base)
                self.R_target2cam_list.append(R_target2cam)
                self.t_target2cam_list.append(t_target2cam)
    
                self.get_logger().info(
                    f"ğŸ“¸ Sample {len(self.R_gripper2base_list)}/{self.num_samples_target} saved!"
                )
    
            if len(self.R_gripper2base_list) >= self.num_samples_target:
                self.compute_handeye()



    # -----------------------------
    # ğŸ“ Hand-Eye ê³„ì‚°
    # -----------------------------
    def compute_handeye(self):
        self.get_logger().info('ğŸ“ Running cv2.calibrateHandEye (GridBoard)...')

        Rg = self.R_gripper2base_list
        tg = self.t_gripper2base_list
        Rt = self.R_target2cam_list
        tt = self.t_target2cam_list

        R_cam2gripper, t_cam2gripper = cv2.calibrateHandEye(
            Rg, tg, Rt, tt, method=cv2.CALIB_HAND_EYE_TSAI
        )

        T_tcp_cam = np.eye(4)
        T_tcp_cam[0:3, 0:3] = R_cam2gripper
        T_tcp_cam[0:3, 3] = t_cam2gripper.flatten()

        self.get_logger().info('ğŸ‰ Hand-Eye calibration result (T_tcp_cam = tcp <- cam):')
        self.get_logger().info('\nT_tcp_cam =\n' + str(T_tcp_cam))

        tx, ty, tz = t_cam2gripper.flatten()
        roll, pitch, yaw = self.rotationMatrixToRPY(R_cam2gripper)

        self.get_logger().info(
            f'\nğŸ‘‰ Use this in static_transform_publisher (tool0 -> camera_link)\n'
            f'xyz = ({tx:.4f}, {ty:.4f}, {tz:.4f}) [m]\n'
            f'rpy = ({roll:.4f}, {pitch:.4f}, {yaw:.4f}) [rad]\n'
        )

    # -----------------------------
    # ğŸ”§ ì¿¼í„°ë‹ˆì–¸ â†’ 4x4 í–‰ë ¬
    # -----------------------------
    def quaternion_to_matrix(self, q):
        x, y, z, w = q
        n = x*x + y*y + z*z + w*w
        if n < 1e-8:
            return np.eye(4, dtype=np.float64)
        s = 2.0 / n
        xx, yy, zz = x*x*s, y*y*s, z*z*s
        xy, xz, yz = x*y*s, x*z*s, y*z*s
        wx, wy, wz = w*x*s, w*y*s, w*z*s

        R = np.array([
            [1.0 - (yy + zz),     xy - wz,           xz + wy],
            [xy + wz,             1.0 - (xx + zz),   yz - wx],
            [xz - wy,             yz + wx,           1.0 - (xx + yy)]
        ], dtype=np.float64)

        T = np.eye(4, dtype=np.float64)
        T[0:3, 0:3] = R
        return T

    # -----------------------------
    # ğŸ”§ íšŒì „í–‰ë ¬(3x3) â†’ RPY
    # -----------------------------
    def rotationMatrixToRPY(self, R):
        sy = np.sqrt(R[0, 0] ** 2 + R[1, 0] ** 2)
        singular = sy < 1e-6

        if not singular:
            roll = np.arctan2(R[2, 1], R[2, 2])
            pitch = np.arctan2(-R[2, 0], sy)
            yaw = np.arctan2(R[1, 0], R[0, 0])
        else:
            roll = np.arctan2(-R[1, 2], R[1, 1])
            pitch = np.arctan2(-R[2, 0], sy)
            yaw = 0.0

        return roll, pitch, yaw


def main(args=None):
    rclpy.init(args=args)
    node = HandEyeCalibNode()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
            pass
    node.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
